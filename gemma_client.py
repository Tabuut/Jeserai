import json
import os
import logging
import asyncio
from openai import OpenAI
import base64
from io import BytesIO
import requests

# Setup logging
logger = logging.getLogger(__name__)

class GemmaClient:
    def __init__(self):
        """Initialize Gemma 3 27B client using OpenRouter API."""
        self.api_key = os.getenv('GEMMA_3_27B_API_KEY')
        if not self.api_key:
            raise ValueError("GEMMA_3_27B_API_KEY environment variable is required")
        
        # Configure client for OpenRouter endpoint
        self.client = OpenAI(
            api_key=self.api_key,
            base_url="https://openrouter.ai/api/v1"
        )
        
        # System instruction for Iraqi dialect responses
        self.system_instruction = """Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙŠØªØ­Ø¯Ø« Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ©. Ù…Ù‡Ø§Ù…Ùƒ:

1. Ø±Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ© Ø§Ù„Ø£ØµÙŠÙ„Ø© ÙˆØ§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©
2. Ø§Ø³ØªØ®Ø¯Ù… ÙƒÙ„Ù…Ø§Øª ÙˆØªØ¹Ø§Ø¨ÙŠØ± Ø¹Ø±Ø§Ù‚ÙŠØ© Ù…Ø«Ù„: Ø´Ù„ÙˆÙ†ÙƒØŒ Ù‡Ø³Ù‡ØŒ Ù‡ÙˆØ§ÙŠØ©ØŒ ÙƒÙ„Ø´ØŒ Ø²ÙŠÙ†ØŒ Ø´Ù†ÙˆØŒ ÙˆÙŠÙ†ØŒ ÙƒÙŠÙÙƒ
3. ÙƒÙ† Ù…ÙÙŠØ¯ ÙˆÙ…Ø³Ø§Ø¹Ø¯ ÙÙŠ ÙƒÙ„ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª
4. Ø¥Ø°Ø§ Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø±Ø³Ø§Ù„Ø© ØªØ±ÙˆÙŠØ¬ÙŠØ© Ø£Ùˆ Ø¥Ø¹Ù„Ø§Ù†ÙŠØ©ØŒ Ø§ÙƒØªØ¨ Ù„Ù‡ Ø±Ø³Ø§Ù„Ø© Ø¬Ø°Ø§Ø¨Ø© Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ©
5. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨ÙŠØ±ÙŠØ© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ø¬Ø¹Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø£ÙƒØ«Ø± Ø­ÙŠÙˆÙŠØ©
6. ÙƒÙ† ØµØ¯ÙŠÙ‚ ÙˆÙ…ØªÙÙ‡Ù… ÙˆØ§Ø¬Ø¹Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ù…Ù…ØªØ¹Ø©

ØªØ°ÙƒØ±: Ø§Ù„Ù‡Ø¯Ù Ù‡Ùˆ Ø§Ù„ØªØ­Ø¯Ø« Ù…Ø«Ù„ Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠ Ø§Ù„Ø£ØµÙŠÙ„ØŒ ÙØ§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„Ù‡Ø¬Ø© Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆÙ…ÙÙ‡ÙˆÙ…Ø©."""

    async def generate_arabic_response(self, user_message: str) -> str:
        """
        Generate an Arabic response to the user's message using Gemma 3 27B.
        
        Args:
            user_message: The user's input message
            
        Returns:
            Arabic response from Gemma 3 27B
        """
        try:
            logger.info(f"Sending request to Gemma 3 27B for message: {user_message[:50]}...")
            
            response = self.client.chat.completions.create(
                model="google/gemma-2-27b-it",
                messages=[
                    {"role": "system", "content": self.system_instruction},
                    {"role": "user", "content": user_message}
                ],
                temperature=0.7,
                max_tokens=800,
                extra_headers={
                    "HTTP-Referer": "https://telegram-bot-iraqi",
                    "X-Title": "Iraqi Dialect Bot"
                }
            )
            
            if response.choices and response.choices[0].message.content:
                result = response.choices[0].message.content.strip()
                logger.info("Successfully received response from Gemma 3 27B")
                return result
            else:
                logger.warning(f"Empty response from Gemma 3 27B for prompt: {user_message[:50]}...")
                return "Ù‡Ù„Ø§ ÙˆØºÙ„Ø§! Ø´Ù„ÙˆÙ†ÙƒØŸ ÙƒÙ„Ø´ Ø£Ø³ÙØŒ Ù…Ø§ ÙƒØ¯Ø±Øª Ø§ÙÙ‡Ù… Ø·Ù„Ø¨Ùƒ Ø²ÙŠÙ†. Ù…Ù…ÙƒÙ† ØªÙˆØ¶Ø­Ù„ÙŠ Ø§ÙƒØ«Ø± Ø´ÙˆÙŠØŸ"
                
        except Exception as e:
            logger.error(f"Error generating response from Gemma 3 27B: {e}")
            return "Ù…Ø¹Ø°Ø±Ø©ØŒ ØµØ§Ø± Ø®Ø·Ø£ ÙˆÙ‚Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ. Ø¬Ø±Ø¨ Ù…Ø±Ø© Ø«Ø§Ù†ÙŠØ© Ø¨Ø§Ø¬Ø±."
    
    async def generate_image(self, prompt: str, image_path: str) -> bool:
        """
        Generate an image using a fallback method since Gemma doesn't support image generation.
        For now, we'll return False and handle this in the bot logic.
        
        Args:
            prompt: Description of the image to generate
            image_path: Path where to save the generated image
            
        Returns:
            False since Gemma doesn't support image generation
        """
        logger.warning("Image generation not supported with Gemma 3 27B model")
        return False
    
    async def analyze_image(self, image_data: bytes, user_message: str = "") -> str:
        """
        Analyze an image using Gemma 3 27B (if supported).
        Note: Gemma 3 27B may have limited vision capabilities.
        
        Args:
            image_data: The image data as bytes
            user_message: Optional context message from user
            
        Returns:
            Analysis description in Arabic
        """
        try:
            logger.info("Attempting image analysis with Gemma 3 27B...")
            
            # Convert image to base64
            base64_image = base64.b64encode(image_data).decode('utf-8')
            
            # Create prompt for image analysis
            analysis_prompt = "Ø­Ù„Ù„ Ù‡Ø°Ù‡ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø§Ù„ØªÙØµÙŠÙ„ ÙˆØ§ÙˆØµÙÙ‡Ø§ Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ©. Ø§Ø°ÙƒØ± ÙƒÙ„ Ø´ÙŠ ØªØ´ÙˆÙÙ‡ ÙÙŠÙ‡Ø§ - Ø§Ù„Ø£Ù„ÙˆØ§Ù†ØŒ Ø§Ù„Ø£Ø´Ø®Ø§ØµØŒ Ø§Ù„Ù…ÙƒØ§Ù†ØŒ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ØŒ ÙˆØ§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ù‡Ù…Ø©."
            
            if user_message:
                analysis_prompt += f"\n\nØ§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù‚Ø§Ù„: {user_message}"
            
            # Try to analyze with vision capabilities
            response = self.client.chat.completions.create(
                model="google/gemma-2-27b-it",
                messages=[
                    {
                        "role": "system", 
                        "content": self.system_instruction
                    },
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": analysis_prompt
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}"
                                }
                            }
                        ]
                    }
                ],
                temperature=0.7,
                max_tokens=1000,
            )
            
            if response.choices and response.choices[0].message.content:
                result = response.choices[0].message.content.strip()
                logger.info("Successfully analyzed image with Gemma 3 27B")
                return result
            else:
                logger.warning("Empty response from Gemma 3 27B for image analysis")
                return "Ù…Ø¹Ø°Ø±Ø©ØŒ Ù…Ø§ ÙƒØ¯Ø±Øª Ø§Ø­Ù„Ù„ Ø§Ù„ØµÙˆØ±Ø© Ù‡Ø³Ù‡. Ø¬Ø±Ø¨ Ù…Ø±Ø© Ø«Ø§Ù†ÙŠØ©."
                
        except Exception as e:
            logger.error(f"Error analyzing image with Gemma 3 27B: {e}")
            # Fallback to text-only analysis
            fallback_prompt = f"Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø£Ø±Ø³Ù„ ØµÙˆØ±Ø© ÙˆÙ‚Ø§Ù„: {user_message if user_message else 'Ù…Ø§ Ù‚Ø§Ù„ Ø´ÙŠ'}. Ø±Ø¯ Ø¹Ù„ÙŠÙ‡ Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ© ÙˆØ§Ø¹ØªØ°Ø± Ù„Ù‡ Ø¥Ù†Ùƒ Ù…Ø§ ØªÙƒØ¯Ø± ØªØ´ÙˆÙ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø³ ØªÙƒØ¯Ø± ØªØ³Ø§Ø¹Ø¯Ù‡ Ø¨Ø·Ø±Ù‚ Ø«Ø§Ù†ÙŠØ©."
            
            try:
                response = self.client.chat.completions.create(
                    model="google/gemma-2-27b-it",
                    messages=[
                        {"role": "system", "content": self.system_instruction},
                        {"role": "user", "content": fallback_prompt}
                    ],
                    temperature=0.7,
                    max_tokens=500,
                )
                
                if response.choices and response.choices[0].message.content:
                    return response.choices[0].message.content.strip()
                    
            except Exception as fallback_error:
                logger.error(f"Fallback analysis also failed: {fallback_error}")
            
            return "Ù…Ø¹Ø°Ø±Ø©ØŒ ØµØ§Ø± Ø®Ø·Ø£ ÙˆÙ‚Øª ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©. Ø¬Ø±Ø¨ Ù…Ø±Ø© Ø«Ø§Ù†ÙŠØ©."
    
    async def generate_creative_description(self, user_prompt: str) -> tuple[str, str]:
        """
        Generate creative description with both Arabic response and English prompt.
        
        Args:
            user_prompt: User's description request
            
        Returns:
            Tuple of (arabic_response, english_prompt)
        """
        try:
            logger.info(f"Generating creative description for: {user_prompt[:50]}...")
            
            # First, generate the English technical prompt
            english_prompt_request = f"""
Based on this user request: "{user_prompt}"

Create a detailed, technical English prompt suitable for AI image generation. Include:
- Visual style and artistic elements
- Colors, lighting, and composition
- Technical photography/art terms
- Detailed scene description

Make it comprehensive and specific for best AI image generation results.
Return ONLY the English prompt, nothing else.
"""
            
            english_response = self.client.chat.completions.create(
                model="google/gemma-2-27b-it",
                messages=[
                    {"role": "user", "content": english_prompt_request}
                ],
                temperature=0.8,
                max_tokens=300,
            )
            
            english_prompt = ""
            if english_response.choices and english_response.choices[0].message.content:
                english_prompt = english_response.choices[0].message.content.strip()
            
            # Then generate the Arabic creative description
            arabic_prompt_request = f"""
Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø·Ù„Ø¨ Ù…Ù†Ùƒ: "{user_prompt}"

Ø§ÙƒØªØ¨ ÙˆØµÙ Ø¥Ø¨Ø¯Ø§Ø¹ÙŠ Ø¬Ù…ÙŠÙ„ ÙˆØ®ÙŠØ§Ù„ÙŠ Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ© ÙŠØµÙ Ø§Ù„ØµÙˆØ±Ø© Ø£Ùˆ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„ÙŠ Ø·Ù„Ø¨Ù‡ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù….
Ø§Ø³ØªØ®Ø¯Ù… ÙƒÙ„Ù…Ø§Øª Ø´Ø§Ø¹Ø±ÙŠØ© ÙˆØªØ¹Ø§Ø¨ÙŠØ± Ø¹Ø±Ø§Ù‚ÙŠØ© Ø¬Ù…ÙŠÙ„Ø© ÙˆØ§Ø¬Ø¹Ù„ Ø§Ù„ÙˆØµÙ Ø­ÙŠÙˆÙŠ ÙˆÙ…Ù„ÙŠØ¡ Ø¨Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø±Ø§Ø¦Ø¹Ø©.
Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨ÙŠØ±ÙŠØ© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ø¬Ø¹Ù„ Ø§Ù„ÙˆØµÙ Ø£ÙƒØ«Ø± Ø¬Ù…Ø§Ù„Ø§Ù‹.
"""
            
            arabic_response = self.client.chat.completions.create(
                model="google/gemma-2-27b-it",
                messages=[
                    {"role": "system", "content": self.system_instruction},
                    {"role": "user", "content": arabic_prompt_request}
                ],
                temperature=0.8,
                max_tokens=500,
            )
            
            arabic_description = ""
            if arabic_response.choices and arabic_response.choices[0].message.content:
                arabic_description = arabic_response.choices[0].message.content.strip()
            else:
                arabic_description = "ÙˆØµÙ Ø¥Ø¨Ø¯Ø§Ø¹ÙŠ Ø±Ø§Ø¦Ø¹ Ù„Ù„ØµÙˆØ±Ø© Ø§Ù„ÙŠ Ø·Ù„Ø¨ØªÙ‡Ø§! ðŸŽ¨âœ¨"
            
            logger.info("Successfully generated creative description with Gemma 3 27B")
            return arabic_description, english_prompt
            
        except Exception as e:
            logger.error(f"Error generating creative description with Gemma 3 27B: {e}")
            fallback_arabic = "Ù…Ø¹Ø°Ø±Ø©ØŒ Ù…Ø§ ÙƒØ¯Ø±Øª Ø§Ø³ÙˆÙŠ Ø§Ù„ÙˆØµÙ Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹ÙŠ Ù‡Ø³Ù‡. Ø¬Ø±Ø¨ Ù…Ø±Ø© Ø«Ø§Ù†ÙŠØ©! ðŸŽ¨"
            fallback_english = f"Create an image of: {user_prompt}"
            return fallback_arabic, fallback_english
    
    async def translate_text(self, text: str, target_language: str) -> str:
        """
        Translate text using Gemma 3 27B.
        
        Args:
            text: Text to translate
            target_language: 'english' or 'arabic'
            
        Returns:
            Translated text
        """
        try:
            logger.info(f"Translating text to {target_language}: {text[:50]}...")
            
            if target_language.lower() == 'english':
                prompt = f"Translate this Arabic text to clear, natural English:\n\n{text}"
            else:
                prompt = f"Translate this English text to natural Iraqi Arabic dialect:\n\n{text}"
            
            response = self.client.chat.completions.create(
                model="google/gemma-2-27b-it",
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=500,
            )
            
            if response.choices and response.choices[0].message.content:
                result = response.choices[0].message.content.strip()
                logger.info(f"Successfully translated text to {target_language}")
                return result
            else:
                logger.warning(f"Empty response from Gemma 3 27B for translation to {target_language}")
                return "Ù…Ø¹Ø°Ø±Ø©ØŒ Ù…Ø§ ÙƒØ¯Ø±Øª Ø§ØªØ±Ø¬Ù… Ø§Ù„Ù†Øµ Ù‡Ø³Ù‡. Ø¬Ø±Ø¨ Ù…Ø±Ø© Ø«Ø§Ù†ÙŠØ©."
                
        except Exception as e:
            logger.error(f"Error translating text with Gemma 3 27B: {e}")
            return "Ù…Ø¹Ø°Ø±Ø©ØŒ ØµØ§Ø± Ø®Ø·Ø£ Ø¨Ø§Ù„ØªØ±Ø¬Ù…Ø©. Ø¬Ø±Ø¨ Ù…Ø±Ø© Ø«Ø§Ù†ÙŠØ©."