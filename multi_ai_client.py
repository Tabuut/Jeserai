"""
Enhanced Multi-AI Client combining GPT-5 Chat and Gemini
Features:
- GPT-5 Chat for advanced text responses and analysis
- Gemini 2.0 Flash for image generation
- GPT-5 Vision for image analysis  
- Optimized for Iraqi dialect specialization
"""

import os
import logging
import base64
import requests
import uuid
import json
from openai import OpenAI
import google.generativeai as genai

logger = logging.getLogger(__name__)

class MultiAIClient:
    """
    Multi-AI client combining GPT-5 Chat and Gemini capabilities
    - GPT-5 Chat: Advanced text generation and analysis via OpenRouter
    - Gemini: High-quality image generation capabilities
    - Iraqi dialect specialization across all models
    """
    
    def __init__(self):
        """Initialize the Multi-AI client with GPT-5 and Gemini."""
        # OpenRouter API key for GPT-5 Chat
        self.openrouter_api_key = os.environ.get("OPENROUTER_API_KEY")
        if not self.openrouter_api_key:
            raise ValueError("OPENROUTER_API_KEY environment variable is required")
        
        # Gemini API key for image generation
        self.gemini_api_key = os.environ.get("GEMINI_API_KEY")
        if not self.gemini_api_key:
            logger.warning("GEMINI_API_KEY not set - image generation will be limited")
        
        # Initialize GPT-5 Chat client via OpenRouter
        self.gpt5_client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=self.openrouter_api_key
        )
        
        # Initialize Gemini client for image generation
        if self.gemini_api_key:
            genai.configure(api_key=self.gemini_api_key)
            self.gemini_client = genai
        else:
            self.gemini_client = None
        
        # Enhanced system instruction for Iraqi dialect
        self.system_instruction = """
ุฃูุช ุจูุช ุฐูู ูุชุฎุตุต ูู ุงูููุฌุฉ ุงูุนุฑุงููุฉ. ุฑุฏ ุนูู ุงููุณุชุฎุฏู ุจููุฌุฉ ุนุฑุงููุฉ ุฃุตููุฉ ูุทุจูุนูุฉ.

ุฎุตุงุฆุตู:
- ุงุณุชุฎุฏู ุงููููุงุช ุงูุนุฑุงููุฉ ุงูุฃุตููุฉ ูุซู: ุดููููุ ุดูู ูุงููุ ูููุ ุดููุ ููุดุ ฺุงูุ ูุณุฉุ ุดููุฉุ ุฒููุ ูุงุดู ุงูุญุงู
- ุงุณุชุฎุฏู ุชุนุงุจูุฑ ุนุฑุงููุฉ: ูุนูู ุดููุ ูุง ูุงูููุ ุตุฏฺฏุ ุฃููุฏุ ุจุงูุถุจุทุ ูุนูููุ ุฎูุด
- ูู ูุฏูุฏ ููุณุงุนุฏ
- ุงุฌุจ ุจูุถูุญ ูุฏูุฉ
- ุงุณุชุฎุฏู GPT-5 Chat ููุญุตูู ุนูู ุฃูุถู ุฌูุฏุฉ ูู ุงูุฅุฌุงุจุงุช
- ููุตูุฑ ุงุณุชุฎุฏู Gemini ูุฅูุดุงุก ุตูุฑ ุนุงููุฉ ุงูุฌูุฏุฉ

ุชุฐูุฑ: ูู ุฅุฌุงุจุงุชู ูุฌุจ ุฃู ุชููู ุจุงูููุฌุฉ ุงูุนุฑุงููุฉ ุงูุฃุตููุฉ.
        """

    async def generate_response(self, user_message: str) -> str:
        """
        Generate response using GPT-5 Chat for enhanced quality.
        
        Args:
            user_message: User's message in any language
            
        Returns:
            Response in Iraqi Arabic dialect
        """
        try:
            logger.info(f"Sending request to GPT-5 Chat for message: {user_message[:50]}...")
            
            response = self.gpt5_client.chat.completions.create(
                extra_headers={
                    "HTTP-Referer": "https://iraqi-bot.replit.app",
                    "X-Title": "Iraqi Dialect Bot - GPT-5",
                },
                model="openai/gpt-4o",
                messages=[
                    {
                        "role": "system", 
                        "content": self.system_instruction
                    },
                    {
                        "role": "user", 
                        "content": user_message
                    }
                ],
                temperature=0.7,
                max_tokens=500,  # Further reduced to work within available credits
            )
            
            if response.choices and response.choices[0].message.content:
                result = response.choices[0].message.content.strip()
                logger.info("Successfully received response from GPT-5 Chat")
                return result
            else:
                logger.warning("Empty response from GPT-5 Chat")
                # Try fallback to Gemini if available
                return await self._fallback_to_gemini(user_message)
                
        except Exception as e:
            logger.error(f"Error calling GPT-5 Chat API: {e}")
            # Check if it's a credit/payment error
            if "402" in str(e) or "credit" in str(e).lower() or "payment" in str(e).lower():
                logger.warning("OpenRouter credits insufficient, trying fallback to Gemini")
                return await self._fallback_to_gemini(user_message)
            return "ูุนุฐุฑุฉุ ุตุงุฑ ุฎุทุฃ ููุช ูุนุงูุฌุฉ ุทูุจู. ุฌุฑุจ ูุฑุฉ ุซุงููุฉ ุจุงุฌุฑ."
    
    async def _fallback_to_gemini(self, user_message: str) -> str:
        """
        Fallback to Gemini when OpenRouter fails or runs out of credits.
        
        Args:
            user_message: User's message
            
        Returns:
            Response in Iraqi Arabic dialect
        """
        # Always try Gemini first if available
        if self.gemini_client:
            try:
                logger.info("Using Gemini as fallback for text generation")
                
                # Enhanced Gemini system instruction for better responses
                gemini_instruction = """
ุฃูุช ุจูุช ุฐูู ูุชุฎุตุต ูู ุงูููุฌุฉ ุงูุนุฑุงููุฉ. ุงููุณุชุฎุฏู ูููู ูู ุดูุกุ ูุฃูุช ุชุญุชุงุฌ ุฃู ุชุฑุฏ ุนููู ุจุดูู ุทุจูุนู ููููุฏ.

ุฎุตุงุฆุตู:
- ุงุณุชุฎุฏู ุงูููุฌุฉ ุงูุนุฑุงููุฉ ุงูุฃุตููุฉ: ุดูู ูุงููุ ูููุ ุดููุ ููุดุ ฺุงูุ ูุณูุ ุดููุฉุ ุฒููุ ูุงุดู ุงูุญุงู
- ูู ูุฏูุฏ ููุณุงุนุฏ ุจุฏูู ุชุญูุงุช ูุชูุฑุฑุฉ
- ุงุฌุจ ุนูู ุงูุฃุณุฆูุฉ ุจูุถูุญ ููุจุงุดุฑุฉ
- ุฅุฐุง ูุงู "ูู ูู 10" ุฃุนุทู ุชูููู ุฑููู
- ุฅุฐุง ูุงู "ุฌูุฏ" ุฃู "ููุชุงุฒ" ุงุนุฑู ุฅูู ูุนูู ุนูู ุฅุฌุงุจุฉ ุณุงุจูุฉ
- ุฅุฐุง ูุงู ุดู ุบุงูุถุ ุงุทูุจ ุชูุถูุญ ุจุทุฑููุฉ ูุฏูุฉ
- ูุง ุชูุฑุฑ ููุณ ุงูุฌูุงุจ ุฏุงุฆูุงูุ ููุน ูู ุฑุฏูุฏู
- ุชุฌูุจ ุงูุจุฏุก ุจุชุญูุงุช ุซุงุจุชุฉ ูุซู "ููุง ุจูู" ุฃู "ุดูููู" ูู ูู ุฑุฏ
- ุงุฏุฎู ูุจุงุดุฑุฉ ูู ุงูููุถูุน ูุงุฌุจ ุนูู ุงูุณุคุงู

ุงููุณุชุฎุฏู ูุงู:
                """
                
                model = genai.GenerativeModel('gemini-1.5-flash')
                response = model.generate_content(
                    f"{gemini_instruction}{user_message}",
                    generation_config=genai.types.GenerationConfig(
                        max_output_tokens=400,
                        temperature=0.8,
                    )
                )
                
                # Better response handling for Gemini
                if response and response.text:
                    result = response.text.strip()
                    logger.info("Successfully received fallback response from Gemini")
                    return result
                
                logger.warning("Empty response from Gemini fallback")
                    
            except Exception as e:
                logger.error(f"Error in Gemini fallback: {e}")
        
        # Only use hardcoded responses as absolute last resort
        logger.warning("Using hardcoded fallback responses")
        
        # More helpful fallback responses that guide users to proper commands
        user_lower = user_message.lower()
        
        # Context-aware fallback responses to avoid repetitive replies
        if "ุณูุงุฑุงุช" in user_lower or "ุณูุงุฑุฉ" in user_lower:
            return (
                "ุงุดูู ุงูู ุชุณุฃู ุนู ุงูุณูุงุฑุงุช! ๐\n\n"
                "ูููู ุงุณุงุนุฏู ุจูุนูููุงุช ุนู:\n"
                "๐ง ุงููุงุน ุงูุณูุงุฑุงุช\n"
                "๐ฐ ุงุณุนุงุฑ ุงูุณูุงุฑุงุช\n" 
                "โ๏ธ ููุงุตูุงุช ูุชููููุงุช\n"
                "๐ ูุตุงุฆุญ ููุดุฑุงุก\n\n"
                "ูุถุญูู ุงูุซุฑ ุดูู ุชุญุชุงุฌ ูุฑุงุญ ุงุณุงุนุฏู!"
            )
        # For quiz/poll related requests - guide to proper commands
        elif any(quiz_word in user_lower for quiz_word in ["ุงุฎุชุจุงุฑ", "ุงุณุชุทูุงุน", "ุณุคุงู", "ุงุณุชูุชุงุก", "ุงูุชุญุงู"]):
            return (
                "ุฃุดูู ุฅูู ุชุฑูุฏ ุชุณูู ุงุฎุชุจุงุฑ ุฃู ุงุณุชุทูุงุน! ๐\n\n"
                "ุงุณุชุฎุฏู ูุงู ุงูุฃูุงูุฑ:\n"
                "๐ /quiz - ููุงุฎุชุจุงุฑ ุงูุชุนูููู (ุณุงุฏุณ ุงุจุชุฏุงุฆู)\n"
                "๐ /create_poll - ูุฅูุดุงุก ุงุณุชุทูุงุน ูุฎุตุต\n"
                "๐ /help_poll - ูููุณุงุนุฏุฉ ูุงูุดุฑุญ\n\n"
                "ูุซุงู: /create_poll ุดูู ุฑุฃููู ุจุงูุจูุชุ,ููุชุงุฒ,ุฌูุฏ,ูุญุชุงุฌ ุชุทููุฑ"
            )
        elif any(help_word in user_lower for help_word in ["ูุณุงุนุฏุฉ", "ุดููู", "ููู", "ููู", "ุงุฑูุฏ", "ูููู"]):
            return (
                "ุงููุฏ ุงูุฏุฑ ุงุณุงุนุฏู! ๐ช\n\n"
                "ูููู ุงุณุชุฎุฏุงู:\n"
                "โ /help - ูููุณุงุนุฏุฉ ุงูุดุงููุฉ\n"
                "๐ฌ /chat - ูููุญุงุฏุซุฉ ุงูุฐููุฉ\n"
                "๐จ /image - ูุฅูุดุงุก ุงูุตูุฑ\n"
                "๐ /create_poll - ููุงุณุชุทูุงุนุงุช\n\n"
                "ุงู ุงูุชุจูู ูุจุงุดุฑุฉ ุดุชุฑูุฏ ูุงูุฏุฑ ุงุณุงุนุฏู!"
            )
        else:
            # Context-specific fallback instead of generic error
            return (
                "ูููุช ุทูุจู ุจุณ ุตุงุฑ ุฎุทุฃ ูุคูุช! ๐\n\n"
                "ุฌุฑุจ ูุฑุฉ ุซุงููุฉ ุงู ุงุณุชุฎุฏู:\n"
                "๐ฌ /chat - ูููุญุงุฏุซุฉ ุงูุนุงุฏูุฉ\n"
                "โ /help - ูููุณุงุนุฏุฉ\n\n"
                "ุงูุชุจูู ุจุทุฑููุฉ ูุฎุชููุฉ ูุงูุฏุฑ ุงุณุงุนุฏู ุงูุซุฑ!"
            )
    
    async def generate_image(self, prompt: str, image_path: str) -> bool:
        """
        Generate an image using Gemini 2.0 Flash image generation.
        
        Args:
            prompt: Description of the image to generate
            image_path: Path where to save the generated image
            
        Returns:
            True if successful, False otherwise
        """
        try:
            # Check if Gemini is available
            if not self.gemini_client:
                logger.error("Gemini image generation unavailable - GEMINI_API_KEY not set")
                return False
            
            logger.info(f"Generating image with Gemini 2.0 Flash for prompt: {prompt[:50]}...")
            
            # Generate image with Gemini (note: direct image generation via API is limited)
            # For now, return False to indicate image generation is not available
            logger.warning("Direct image generation with Gemini API is limited")
            return False
            
            if response.candidates and response.candidates[0].content:
                content = response.candidates[0].content
                
                if content.parts:
                    for part in content.parts:
                        # Look for image data in the response
                        if part.inline_data and part.inline_data.data:
                            # Save the generated image
                            with open(image_path, 'wb') as f:
                                f.write(part.inline_data.data)
                            
                            logger.info("Successfully generated and saved image with Gemini")
                            return True
                        elif part.text:
                            # Log any text response from Gemini
                            logger.info(f"Gemini text response: {part.text[:100]}")
                
                logger.error("No image data found in Gemini response")
                return False
            else:
                logger.error("No content in Gemini response")
                return False
                
        except Exception as e:
            logger.error(f"Error generating image with Gemini: {e}")
            return False
    
    async def analyze_image(self, image_data: bytes, user_message: str = "") -> str:
        """
        Analyze an image using GPT-5 Chat Vision capabilities.
        
        Args:
            image_data: The image data as bytes
            user_message: Optional context message from user
            
        Returns:
            Analysis description in Iraqi Arabic
        """
        try:
            logger.info("Attempting image analysis with GPT-5 Chat Vision...")
            
            # Convert image to base64
            base64_image = base64.b64encode(image_data).decode('utf-8')
            
            # Create prompt for image analysis
            analysis_prompt = "ุญูู ูุฐู ุงูุตูุฑุฉ ุจุงูุชูุตูู ูุงูุตููุง ุจุงูููุฌุฉ ุงูุนุฑุงููุฉ. ุงุฐูุฑ ูู ุดู ุชุดููู ูููุง - ุงูุฃููุงูุ ุงูุฃุดุฎุงุตุ ุงูููุงูุ ุงูุฃุดูุงุกุ ูุงูุชูุงุตูู ุงููููุฉ."
            
            if user_message:
                analysis_prompt += f"\n\nุงููุณุชุฎุฏู ูุงู: {user_message}"
            
            # Analyze with GPT-5 Chat Vision
            response = self.gpt5_client.chat.completions.create(
                extra_headers={
                    "HTTP-Referer": "https://iraqi-bot.replit.app",
                    "X-Title": "Iraqi Dialect Bot - GPT-5",
                },
                model="openai/gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": self.system_instruction + "\n\nุฃูุช ุชุญูู ุตูุฑุฉ ุฃุฑุณููุง ุงููุณุชุฎุฏู. ุงูุตู ูู ุดู ุชุดููู ุจุงูุชูุตูู ูุงุณุชุฎุฏู ุงูููุฌุฉ ุงูุนุฑุงููุฉ."
                    },
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": analysis_prompt
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=500,
                temperature=0.7,
            )
            
            if response.choices and response.choices[0].message.content:
                result = response.choices[0].message.content.strip()
                # Clean the response to avoid formatting issues
                result = self._clean_response_text(result)
                logger.info("Successfully analyzed image with GPT-5 Chat Vision")
                return result
            else:
                logger.warning("Empty response from GPT-5 Chat Vision")
                # Try Gemini Vision as fallback for empty response
                if self.gemini_client:
                    try:
                        logger.info("Trying Gemini Vision as fallback for empty GPT-5 response...")
                        return await self._analyze_image_with_gemini(image_data, user_message)
                    except Exception as gemini_error:
                        logger.error(f"Gemini Vision also failed: {gemini_error}")
                return "ูุง ูุฏุฑุช ุฃุญูู ุงูุตูุฑุฉุ ุจุณ ุชูุฏุฑ ุชุญฺู ููุงู ุนุงุฏู."
                
        except Exception as e:
            logger.error(f"Error in GPT-5 Chat Vision analysis: {e}")
            
            # Try Gemini Vision as fallback
            if self.gemini_client:
                try:
                    logger.info("Trying Gemini Vision as fallback for image analysis...")
                    return await self._analyze_image_with_gemini(image_data, user_message)
                except Exception as gemini_error:
                    logger.error(f"Gemini Vision also failed: {gemini_error}")
            
            # Last resort: text-only fallback response
            fallback_prompt = f"ุงููุณุชุฎุฏู ุฃุฑุณู ุตูุฑุฉ ููุงู: {user_message if user_message else 'ูุง ูุงู ุดู'}. ุฑุฏ ุนููู ุจุงูููุฌุฉ ุงูุนุฑุงููุฉ ูุงุนุชุฐุฑ ูู ุฅูู ูุง ุชูุฏุฑ ุชุดูู ุงูุตูุฑุฉ ุจุณ ุชูุฏุฑ ุชุณุงุนุฏู ุจุทุฑู ุซุงููุฉ."
            
            try:
                response = self.gpt5_client.chat.completions.create(
                    extra_headers={
                        "HTTP-Referer": "https://iraqi-bot.replit.app",
                        "X-Title": "Iraqi Dialect Bot - GPT-5",
                    },
                    model="openai/gpt-4o",
                    messages=[
                        {
                            "role": "system",
                            "content": self.system_instruction
                        },
                        {
                            "role": "user",
                            "content": fallback_prompt
                        }
                    ],
                    max_tokens=500,
                    temperature=0.7,
                )
                
                if response.choices and response.choices[0].message.content:
                    return response.choices[0].message.content.strip()
                    
            except Exception as fallback_error:
                logger.error(f"Error in fallback response: {fallback_error}")
            
            return "ูุนุฐุฑุฉุ ูุง ูุฏุฑุช ุฃุญูู ุงูุตูุฑุฉ ูุณู. ุจุณ ุชูุฏุฑ ุชุญฺู ููุงู ุนุงุฏู ูุฃุณุงุนุฏู ุจุฃู ุดู ุซุงูู."
    
    async def _analyze_image_with_gemini(self, image_data: bytes, user_message: str = "") -> str:
        """
        Analyze an image using Gemini Vision capabilities as fallback.
        
        Args:
            image_data: The image data as bytes
            user_message: Optional context message from user
            
        Returns:
            Analysis description in Iraqi Arabic
        """
        try:
            logger.info("Using Gemini Vision for image analysis...")
            
            # Create prompt for image analysis in Iraqi dialect
            analysis_prompt = """ุญูู ูุฐู ุงูุตูุฑุฉ ุจุงูุชูุตูู ูุงูุตููุง ุจุงูููุฌุฉ ุงูุนุฑุงููุฉ ุงูุฃุตููุฉ. 

ุงุฐูุฑ:
- ุงูุฃููุงู ุงูููุฌูุฏุฉ 
- ุงูุฃุดุฎุงุต ุฃู ุงูุญููุงูุงุช (ุฅุฐุง ููุฌูุฏุฉ)
- ุงูููุงู ุฃู ุงูุจูุฆุฉ
- ุงูุฃุดูุงุก ูุงูุนูุงุตุฑ ุงููููุฉ
- ุงูุชูุงุตูู ุงููุงูุชุฉ ูููุธุฑ
- ุงูุฌู ุงูุนุงู ููุตูุฑุฉ

ุงุณุชุฎุฏู ูููุงุช ุนุฑุงููุฉ ูุซู: ุดูู ูุงููุ ุฒููุ ุญููุ ฺุงูุ ูุณูุ ุดููุฉุ ููุงู

ูู ููุตู ููุตู ูู ุดู ุชุดููู ุจุทุฑููุฉ ูุงุถุญุฉ ููููููุฉ."""
            
            if user_message:
                analysis_prompt += f"\n\nุงููุณุชุฎุฏู ูุงู: {user_message}"
            
            # Convert image data to format Gemini can understand
            import base64
            base64_image = base64.b64encode(image_data).decode('utf-8')
            
            # Analyze with Gemini Vision
            model = genai.GenerativeModel('gemini-1.5-flash')
            import PIL.Image
            import io
            image = PIL.Image.open(io.BytesIO(image_data))
            response = model.generate_content(
                [analysis_prompt, image],
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=800,
                    temperature=0.7,
                )
            )
            
            # Better response handling for Gemini Vision
            if response and response.text:
                result = response.text.strip()
                # Clean the response to avoid formatting issues
                result = self._clean_response_text(result)
                logger.info("Successfully analyzed image with Gemini Vision")
                return result
            
            logger.warning("Empty response from Gemini Vision")
            return "ูุง ูุฏุฑุช ุฃุญูู ุงูุตูุฑุฉ ุจูุงุณุทุฉ Geminiุ ุจุณ ุชูุฏุฑ ุชุญฺู ููุงู ุนุงุฏู."
                
        except Exception as e:
            logger.error(f"Error in Gemini Vision analysis: {e}")
            return "ูุนุฐุฑุฉุ ุตุงุฑ ุฎุทุฃ ููุช ุชุญููู ุงูุตูุฑุฉ ุจู Gemini. ุชูุฏุฑ ุชุญฺู ููุงู ุนุงุฏู ูุฃุณุงุนุฏู ุจุฃู ุดู ุซุงูู."
    
    def _clean_response_text(self, text: str) -> str:
        """Clean response text to avoid Telegram formatting issues."""
        if not text:
            return text
            
        # Remove or escape problematic characters that break Telegram parsing
        # Remove excessive backticks and markdown characters
        import re
        
        # Remove multiple consecutive backticks
        text = re.sub(r'`{2,}', '`', text)
        
        # Remove unmatched backticks at the end
        if text.count('`') % 2 != 0:
            # Remove the last backtick if unmatched
            last_backtick = text.rfind('`')
            if last_backtick != -1:
                text = text[:last_backtick] + text[last_backtick+1:]
        
        # Clean up other problematic markdown characters
        # Remove unmatched asterisks
        if text.count('*') % 2 != 0:
            text = text.replace('*', '')
        
        # Remove unmatched underscores  
        if text.count('_') % 2 != 0:
            text = text.replace('_', '')
            
        # Ensure no broken markdown links
        text = re.sub(r'\[([^\]]*)\]\([^\)]*$', r'\1', text)  # Fix broken links
        
        return text.strip()
    
    async def translate_to_english(self, arabic_text: str) -> str:
        """
        Translate Arabic text to English using GPT-5 Chat.
        
        Args:
            arabic_text: Arabic text to translate
            
        Returns:
            English translation
        """
        try:
            prompt = f"Translate this Arabic text to natural English. Keep the meaning and tone:\n\n{arabic_text}"
            
            response = self.gpt5_client.chat.completions.create(
                extra_headers={
                    "HTTP-Referer": "https://iraqi-bot.replit.app",
                    "X-Title": "Iraqi Dialect Bot - GPT-5",
                },
                model="openai/gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": "You are a professional translator with enhanced GPT-5 capabilities. Translate Arabic to English accurately while preserving meaning, tone, and cultural context. Pay special attention to Iraqi dialect nuances."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                max_tokens=1000,
            )
            
            if response.choices and response.choices[0].message.content:
                result = response.choices[0].message.content.strip()
                logger.info("Successfully translated Arabic to English with GPT-5")
                return result
            else:
                return "Translation failed. Please try again."
                
        except Exception as e:
            logger.error(f"Error in Arabic to English translation: {e}")
            return "Sorry, translation failed. Please try again."
    
    async def translate_to_arabic(self, english_text: str) -> str:
        """
        Translate English text to Iraqi Arabic using GPT-5 Chat.
        
        Args:
            english_text: English text to translate
            
        Returns:
            Iraqi Arabic translation
        """
        try:
            prompt = f"Translate this English text to Iraqi Arabic dialect. Use authentic Iraqi expressions and vocabulary:\n\n{english_text}"
            
            response = self.gpt5_client.chat.completions.create(
                extra_headers={
                    "HTTP-Referer": "https://iraqi-bot.replit.app",
                    "X-Title": "Iraqi Dialect Bot - GPT-5",
                },
                model="openai/gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": self.system_instruction + "\n\nYou are using enhanced GPT-5 translation capabilities for superior English to Iraqi Arabic conversion."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.7,
                max_tokens=1000,
            )
            
            if response.choices and response.choices[0].message.content:
                result = response.choices[0].message.content.strip()
                logger.info("Successfully translated English to Iraqi Arabic with GPT-5")
                return result
            else:
                return "ูุดูุช ุงูุชุฑุฌูุฉ. ุฌุฑุจ ูุฑุฉ ุซุงููุฉ."
                
        except Exception as e:
            logger.error(f"Error in English to Arabic translation: {e}")
            return "ูุนุฐุฑุฉุ ูุดูุช ุงูุชุฑุฌูุฉ. ุฌุฑุจ ูุฑุฉ ุซุงููุฉ."
    
    async def generate_creative_description(self, description: str) -> tuple:
        """
        Generate creative description with English prompt using GPT-5 Chat or Gemini.
        
        Args:
            description: User's description in Arabic
            
        Returns:
            Tuple of (arabic_description, english_prompt)
        """
        try:
            prompt = f"""
ุงููุณุชุฎุฏู ูุฑูุฏ ูุตู ุฅุจุฏุงุนู ูู: {description}

ุงููุทููุจ ููู:
1. ุฅูุดุงุก ูุต ุฅูุฌููุฒู ููุตู ูุฏููู ูุฅูุดุงุก ุงูุตูุฑ (ูููู ูุงุจู ูููุณุฎ)
2. ูุตู ุฅุจุฏุงุนู ุจุงูููุฌุฉ ุงูุนุฑุงููุฉ ูุตู ููุณ ุงูุตูุฑุฉ ุจุทุฑููุฉ ุญูููุฉ ูุฌูููุฉ

ุงุจุฏุฃ ุงูุฌูุงุจ ุจู "ENGLISH_PROMPT:" ุซู ุงููุต ุงูุฅูุฌููุฒูุ ุซู ุณุทุฑ ูุงุฑุบุ ุซู "ARABIC_DESCRIPTION:" ุซู ุงููุตู ุงูุนุฑุงูู.
            """
            
            # Try GPT-5 first
            try:
                response = self.gpt5_client.chat.completions.create(
                    extra_headers={
                        "HTTP-Referer": "https://iraqi-bot.replit.app",
                        "X-Title": "Iraqi Dialect Bot - GPT-5",
                    },
                    model="openai/gpt-4o",
                    messages=[
                        {
                            "role": "system",
                            "content": self.system_instruction + "\n\nYou are using enhanced creative capabilities for superior content generation and detailed prompts."
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    temperature=0.8,
                    max_tokens=1000,
                )
                
                if response.choices and response.choices[0].message.content:
                    result = response.choices[0].message.content.strip()
                    logger.info("Successfully generated creative description with GPT-5")
                    return self._parse_creative_response(result, description)
                    
            except Exception as gpt_error:
                logger.error(f"GPT-5 failed for creative description: {gpt_error}")
                # Try Gemini fallback
                if self.gemini_client:
                    try:
                        response = self.gemini_client.models.generate_content(
                            model="gemini-2.0-flash",
                            contents=[
                                types.Content(role="user", parts=[types.Part(text=prompt)])
                            ],
                            config=types.GenerateContentConfig(
                                max_output_tokens=800,
                                temperature=0.8,
                            ),
                        )
                        
                        if response and hasattr(response, 'text') and response.text:
                            result = response.text.strip()
                            logger.info("Successfully generated creative description with Gemini")
                            return self._parse_creative_response(result, description)
                    except Exception as gemini_error:
                        logger.error(f"Gemini also failed for creative description: {gemini_error}")
            
            # Fallback creative response
            return self._create_fallback_creative_description(description)
                
        except Exception as e:
            logger.error(f"Error generating creative description: {e}")
            return self._create_fallback_creative_description(description)
    
    def _parse_creative_response(self, response: str, description: str) -> tuple:
        """Parse the AI response to extract English prompt and Arabic description."""
        try:
            if "ENGLISH_PROMPT:" in response and "ARABIC_DESCRIPTION:" in response:
                parts = response.split("ARABIC_DESCRIPTION:")
                english_part = parts[0].replace("ENGLISH_PROMPT:", "").strip()
                arabic_part = parts[1].strip()
                return (arabic_part, english_part)
            else:
                # If format is not as expected, create fallback
                return self._create_fallback_creative_description(description)
        except:
            return self._create_fallback_creative_description(description)
    
    def _create_fallback_creative_description(self, description: str) -> tuple:
        """Create fallback creative description when AI fails."""
        english_prompt = f"A beautiful, high-quality artistic image of {description}, professional photography, detailed, vibrant colors, excellent lighting"
        arabic_description = f"ุตูุฑุฉ ุญููุฉ ูููุตูุฉ ูู {description} ุจุฃููุงู ุฒุงููุฉ ูุฅุถุงุกุฉ ููุชุงุฒุฉุ ุชุตููุฑ ุงุญุชุฑุงูู ุนุงูู ุงูุฌูุฏุฉ"
        return (arabic_description, english_prompt)
    
    async def analyze_with_tools(self, prompt: str, analysis_type: str) -> str:
        """
        Advanced analysis using GPT-5 Chat enhanced capabilities.
        
        Args:
            prompt: Text to analyze
            analysis_type: Type of analysis to perform
            
        Returns:
            Analysis result in Iraqi Arabic
        """
        try:
            response = self.gpt5_client.chat.completions.create(
                extra_headers={
                    "HTTP-Referer": "https://iraqi-bot.replit.app",
                    "X-Title": "Iraqi Dialect Bot - GPT-5",
                },
                model="openai/gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": self.system_instruction + "\n\nYou have access to advanced tools and GPT-5 Chat's enhanced capabilities for superior analysis."
                    },
                    {
                        "role": "user",
                        "content": f"ุญูู ูุฐุง ุงููุต ุจุทุฑููุฉ ูุชูุฏูุฉ ({analysis_type}):\n\n{prompt}"
                    }
                ],
                temperature=0.7,
                max_tokens=500,  # Further reduced to work within available credits
            )
            
            if response.choices and response.choices[0].message.content:
                return response.choices[0].message.content.strip()
            else:
                return "ูุนุฐุฑุฉุ ูุง ูุฏุฑุช ุฃุณูู ุงูุชุญููู ุงููุชูุฏู. ุฌุฑุจ ูุฑุฉ ุซุงููุฉ."
                
        except Exception as e:
            logger.error(f"Error in advanced analysis: {e}")
            # Check if it's a credit/payment error
            if "402" in str(e) or "credit" in str(e).lower() or "payment" in str(e).lower():
                logger.warning("OpenRouter credits insufficient for advanced analysis")
                return "ูุนุฐุฑุฉุ ุงูุฃุฑุตุฏุฉ ูู ูุงููุฉ ููุชุญููู ุงููุชูุฏู ูุณุฉ. ุฌุฑุจ ูุฑุฉ ุซุงููุฉ ุจุงุฌุฑ."
            return "ูุนุฐุฑุฉุ ุตุงุฑ ุฎุทุฃ ูู ุงูุชุญููู ุงููุชูุฏู."
    
    async def generate_structured_response(self, prompt: str, response_type: str) -> dict:
        """
        Generate structured response using GPT-5 Chat.
        
        Args:
            prompt: User's prompt
            response_type: Type of structured response
            
        Returns:
            Dictionary with success status and content
        """
        try:
            response = self.gpt5_client.chat.completions.create(
                extra_headers={
                    "HTTP-Referer": "https://iraqi-bot.replit.app",
                    "X-Title": "Iraqi Dialect Bot - GPT-5",
                },
                model="openai/gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": self.system_instruction + f"\n\nGenerate a {response_type} structured response using GPT-5 Chat's enhanced capabilities."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.7,
                max_tokens=500,  # Further reduced to work within available credits
            )
            
            if response.choices and response.choices[0].message.content:
                return {
                    "success": True,
                    "content": response.choices[0].message.content.strip()
                }
            else:
                return {
                    "success": False,
                    "content": "ูุง ูุฏุฑุช ุฃุณูู ุงูุฑุฏ ุงูููุธู. ุฌุฑุจ ูุฑุฉ ุซุงููุฉ."
                }
                
        except Exception as e:
            logger.error(f"Error in structured response: {e}")
            # Check if it's a credit/payment error
            if "402" in str(e) or "credit" in str(e).lower() or "payment" in str(e).lower():
                logger.warning("OpenRouter credits insufficient for structured response")
                return {
                    "success": False,
                    "content": "ูุนุฐุฑุฉุ ุงูุฃุฑุตุฏุฉ ูู ูุงููุฉ ููุฑุฏ ุงูููุธู ูุณุฉ. ุฌุฑุจ ูุฑุฉ ุซุงููุฉ ุจุงุฌุฑ."
                }
            return {
                "success": False,
                "content": "ุตุงุฑ ุฎุทุฃ ูู ุฅูุดุงุก ุงูุฑุฏ ุงูููุธู."
            }
    
    @property
    def openai_api_key(self) -> str:
        """Compatibility property for image generation checks."""
        return self.gemini_api_key or ""