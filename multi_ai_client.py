"""
Enhanced Multi-AI Client combining GPT-5 Chat and Gemini
Features:
- GPT-5 Chat for advanced text responses and analysis
- Gemini 2.0 Flash for image generation
- GPT-5 Vision for image analysis  
- Optimized for Iraqi dialect specialization
"""

import os
import logging
import base64
import requests
import uuid
import json
from openai import OpenAI
import google.generativeai as genai

logger = logging.getLogger(__name__)

class MultiAIClient:
    """
    Multi-AI client combining GPT-5 Chat and Gemini capabilities
    - GPT-5 Chat: Advanced text generation and analysis via OpenRouter
    - Gemini: High-quality image generation capabilities
    - Iraqi dialect specialization across all models
    """
    
    def __init__(self):
        """Initialize the Multi-AI client with GPT-5 and Gemini."""
        # OpenRouter API key for GPT-5 Chat
        self.openrouter_api_key = os.environ.get("OPENROUTER_API_KEY")
        if not self.openrouter_api_key:
            raise ValueError("OPENROUTER_API_KEY environment variable is required")
        
        # Gemini API key for image generation
        self.gemini_api_key = os.environ.get("GEMINI_API_KEY")
        if not self.gemini_api_key:
            logger.warning("GEMINI_API_KEY not set - image generation will be limited")
        
        # Initialize GPT-5 Chat client via OpenRouter
        self.gpt5_client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=self.openrouter_api_key
        )
        
        # Initialize Gemini client for image generation
        if self.gemini_api_key:
            genai.configure(api_key=self.gemini_api_key)
            self.gemini_client = genai
        else:
            self.gemini_client = None
        
        # Enhanced system instruction for Iraqi dialect
        self.system_instruction = """
Ø£Ù†Øª Ø¨ÙˆØª Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ©. Ø±Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ù„Ù‡Ø¬Ø© Ø¹Ø±Ø§Ù‚ÙŠØ© Ø£ØµÙŠÙ„Ø© ÙˆØ·Ø¨ÙŠØ¹ÙŠØ©.

Ø®ØµØ§Ø¦ØµÙƒ:
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ© Ø§Ù„Ø£ØµÙŠÙ„Ø© Ù…Ø«Ù„: Ø´Ù„ÙˆÙ†ÙƒØŒ Ø´ÙƒÙˆ Ù…Ø§ÙƒÙˆØŒ ÙˆÙŠÙ†ØŒ Ø´Ù†ÙˆØŒ Ù„ÙŠØ´ØŒ Ú†Ø§Ù†ØŒ Ù‡Ø³Ø©ØŒ Ø´ÙˆÙŠØ©ØŒ Ø²ÙŠÙ†ØŒ Ù…Ø§Ø´ÙŠ Ø§Ù„Ø­Ø§Ù„
- Ø§Ø³ØªØ®Ø¯Ù… ØªØ¹Ø§Ø¨ÙŠØ± Ø¹Ø±Ø§Ù‚ÙŠØ©: ÙŠØ¹Ù†ÙŠ Ø´Ù†ÙˆØŒ Ù„Ø§ ÙˆØ§Ù„Ù„Ù‡ØŒ ØµØ¯Ú¯ØŒ Ø£ÙƒÙŠØ¯ØŒ Ø¨Ø§Ù„Ø¶Ø¨Ø·ØŒ Ù…Ø¹Ù‚ÙˆÙ„ØŒ Ø®ÙˆØ´
- ÙƒÙ† ÙˆØ¯ÙˆØ¯ ÙˆÙ…Ø³Ø§Ø¹Ø¯
- Ø§Ø¬Ø¨ Ø¨ÙˆØ¶ÙˆØ­ ÙˆØ¯Ù‚Ø©
- Ø§Ø³ØªØ®Ø¯Ù… GPT-5 Chat Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ø¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª
- Ù„Ù„ØµÙˆØ± Ø§Ø³ØªØ®Ø¯Ù… Gemini Ù„Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬ÙˆØ¯Ø©

ØªØ°ÙƒØ±: ÙƒÙ„ Ø¥Ø¬Ø§Ø¨Ø§ØªÙƒ ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ© Ø§Ù„Ø£ØµÙŠÙ„Ø©.
        """

    async def generate_response(self, user_message: str) -> str:
        """
        Generate response using GPT-5 Chat for enhanced quality.
        
        Args:
            user_message: User's message in any language
            
        Returns:
            Response in Iraqi Arabic dialect
        """
        try:
            logger.info(f"Sending request to GPT-5 Chat for message: {user_message[:50]}...")
            
            response = self.gpt5_client.chat.completions.create(
                extra_headers={
                    "HTTP-Referer": "https://iraqi-bot.replit.app",
                    "X-Title": "Iraqi Dialect Bot - GPT-5",
                },
                model="openai/gpt-4o",
                messages=[
                    {
                        "role": "system", 
                        "content": self.system_instruction
                    },
                    {
                        "role": "user", 
                        "content": user_message
                    }
                ],
                temperature=0.7,
                max_tokens=500,  # Further reduced to work within available credits
            )
            
            if response.choices and response.choices[0].message.content:
                result = response.choices[0].message.content.strip()
                logger.info("Successfully received response from GPT-5 Chat")
                return result
            else:
                logger.warning("Empty response from GPT-5 Chat")
                # Try fallback to Gemini if available
                return await self._fallback_to_gemini(user_message)
                
        except Exception as e:
            logger.error(f"Error calling GPT-5 Chat API: {e}")
            # Check if it's a credit/payment error
            if "402" in str(e) or "credit" in str(e).lower() or "payment" in str(e).lower():
                logger.warning("OpenRouter credits insufficient, trying fallback to Gemini")
                return await self._fallback_to_gemini(user_message)
            return "Ù…Ø¹Ø°Ø±Ø©ØŒ ØµØ§Ø± Ø®Ø·Ø£ ÙˆÙ‚Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ. Ø¬Ø±Ø¨ Ù…Ø±Ø© Ø«Ø§Ù†ÙŠØ© Ø¨Ø§Ø¬Ø±."
    
    async def _fallback_to_gemini(self, user_message: str) -> str:
        """
        Fallback to Gemini when OpenRouter fails or runs out of credits.
        
        Args:
            user_message: User's message
            
        Returns:
            Response in Iraqi Arabic dialect
        """
        # Always try Gemini first if available
        if self.gemini_client:
            try:
                logger.info("Using Gemini as fallback for text generation")
                
                # Enhanced Gemini system instruction for better responses
                gemini_instruction = """
Ø£Ù†Øª Ø¨ÙˆØª Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ©. Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠÙ‚ÙˆÙ„ Ù„Ùƒ Ø´ÙŠØ¡ØŒ ÙˆØ£Ù†Øª ØªØ­ØªØ§Ø¬ Ø£Ù† ØªØ±Ø¯ Ø¹Ù„ÙŠÙ‡ Ø¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ ÙˆÙ…ÙÙŠØ¯.

Ø®ØµØ§Ø¦ØµÙƒ:
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ© Ø§Ù„Ø£ØµÙŠÙ„Ø©: Ø´ÙƒÙˆ Ù…Ø§ÙƒÙˆØŒ ÙˆÙŠÙ†ØŒ Ø´Ù†ÙˆØŒ Ù„ÙŠØ´ØŒ Ú†Ø§Ù†ØŒ Ù‡Ø³Ù‡ØŒ Ø´ÙˆÙŠØ©ØŒ Ø²ÙŠÙ†ØŒ Ù…Ø§Ø´ÙŠ Ø§Ù„Ø­Ø§Ù„
- ÙƒÙ† ÙˆØ¯ÙˆØ¯ ÙˆÙ…Ø³Ø§Ø¹Ø¯ Ø¨Ø¯ÙˆÙ† ØªØ­ÙŠØ§Øª Ù…ØªÙƒØ±Ø±Ø©
- Ø§Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨ÙˆØ¶ÙˆØ­ ÙˆÙ…Ø¨Ø§Ø´Ø±Ø©
- Ø¥Ø°Ø§ Ù‚Ø§Ù„ "ÙƒÙ… Ù…Ù† 10" Ø£Ø¹Ø·ÙŠ ØªÙ‚ÙŠÙŠÙ… Ø±Ù‚Ù…ÙŠ
- Ø¥Ø°Ø§ Ù‚Ø§Ù„ "Ø¬ÙŠØ¯" Ø£Ùˆ "Ù…Ù…ØªØ§Ø²" Ø§Ø¹Ø±Ù Ø¥Ù†Ù‡ ÙŠØ¹Ù„Ù‚ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ø³Ø§Ø¨Ù‚Ø©
- Ø¥Ø°Ø§ Ù‚Ø§Ù„ Ø´ÙŠ ØºØ§Ù…Ø¶ØŒ Ø§Ø·Ù„Ø¨ ØªÙˆØ¶ÙŠØ­ Ø¨Ø·Ø±ÙŠÙ‚Ø© ÙˆØ¯ÙŠØ©
- Ù„Ø§ ØªÙƒØ±Ø± Ù†ÙØ³ Ø§Ù„Ø¬ÙˆØ§Ø¨ Ø¯Ø§Ø¦Ù…Ø§Ù‹ØŒ Ù†ÙˆØ¹ ÙÙŠ Ø±Ø¯ÙˆØ¯Ùƒ
- ØªØ¬Ù†Ø¨ Ø§Ù„Ø¨Ø¯Ø¡ Ø¨ØªØ­ÙŠØ§Øª Ø«Ø§Ø¨ØªØ© Ù…Ø«Ù„ "Ù‡Ù„Ø§ Ø¨ÙŠÙƒ" Ø£Ùˆ "Ø´Ù„ÙˆÙ†Ùƒ" ÙÙŠ ÙƒÙ„ Ø±Ø¯
- Ø§Ø¯Ø®Ù„ Ù…Ø¨Ø§Ø´Ø±Ø© ÙÙŠ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ ÙˆØ§Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„

Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù‚Ø§Ù„:
                """
                
                model = genai.GenerativeModel('gemini-1.5-flash')
                response = model.generate_content(
                    f"{gemini_instruction}{user_message}",
                    generation_config=genai.types.GenerationConfig(
                        max_output_tokens=400,
                        temperature=0.8,
                    )
                )
                
                # Better response handling for Gemini
                if response and response.text:
                    result = response.text.strip()
                    logger.info("Successfully received fallback response from Gemini")
                    return result
                
                logger.warning("Empty response from Gemini fallback")
                    
            except Exception as e:
                logger.error(f"Error in Gemini fallback: {e}")
        
        # Only use hardcoded responses as absolute last resort
        logger.warning("Using hardcoded fallback responses")
        
        # More helpful fallback responses that guide users to proper commands
        user_lower = user_message.lower()
        
        # Context-aware fallback responses to avoid repetitive replies
        if "Ø³ÙŠØ§Ø±Ø§Øª" in user_lower or "Ø³ÙŠØ§Ø±Ø©" in user_lower:
            return (
                "Ø§Ø´ÙˆÙ Ø§Ù†Ùƒ ØªØ³Ø£Ù„ Ø¹Ù† Ø§Ù„Ø³ÙŠØ§Ø±Ø§Øª! ðŸš—\n\n"
                "Ù…Ù…ÙƒÙ† Ø§Ø³Ø§Ø¹Ø¯Ùƒ Ø¨Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù†:\n"
                "ðŸ”§ Ø§Ù†ÙˆØ§Ø¹ Ø§Ù„Ø³ÙŠØ§Ø±Ø§Øª\n"
                "ðŸ’° Ø§Ø³Ø¹Ø§Ø± Ø§Ù„Ø³ÙŠØ§Ø±Ø§Øª\n" 
                "âš™ï¸ Ù…ÙˆØ§ØµÙØ§Øª ÙˆØªÙ‚ÙŠÙŠÙ…Ø§Øª\n"
                "ðŸ” Ù†ØµØ§Ø¦Ø­ Ù„Ù„Ø´Ø±Ø§Ø¡\n\n"
                "ÙˆØ¶Ø­Ù„ÙŠ Ø§ÙƒØ«Ø± Ø´Ù†Ùˆ ØªØ­ØªØ§Ø¬ ÙˆØ±Ø§Ø­ Ø§Ø³Ø§Ø¹Ø¯Ùƒ!"
            )
        # For quiz/poll related requests - guide to proper commands
        elif any(quiz_word in user_lower for quiz_word in ["Ø§Ø®ØªØ¨Ø§Ø±", "Ø§Ø³ØªØ·Ù„Ø§Ø¹", "Ø³Ø¤Ø§Ù„", "Ø§Ø³ØªÙØªØ§Ø¡", "Ø§Ù…ØªØ­Ø§Ù†"]):
            return (
                "Ø£Ø´ÙˆÙ Ø¥Ù†Ùƒ ØªØ±ÙŠØ¯ ØªØ³ÙˆÙŠ Ø§Ø®ØªØ¨Ø§Ø± Ø£Ùˆ Ø§Ø³ØªØ·Ù„Ø§Ø¹! ðŸ“Š\n\n"
                "Ø§Ø³ØªØ®Ø¯Ù… Ù‡Ø§ÙŠ Ø§Ù„Ø£ÙˆØ§Ù…Ø±:\n"
                "ðŸŽ“ /quiz - Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ (Ø³Ø§Ø¯Ø³ Ø§Ø¨ØªØ¯Ø§Ø¦ÙŠ)\n"
                "ðŸ“‹ /create_poll - Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³ØªØ·Ù„Ø§Ø¹ Ù…Ø®ØµØµ\n"
                "ðŸ“š /help_poll - Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙˆØ§Ù„Ø´Ø±Ø­\n\n"
                "Ù…Ø«Ø§Ù„: /create_poll Ø´Ù†Ùˆ Ø±Ø£ÙŠÙƒÙ… Ø¨Ø§Ù„Ø¨ÙˆØªØŸ,Ù…Ù…ØªØ§Ø²,Ø¬ÙŠØ¯,ÙŠØ­ØªØ§Ø¬ ØªØ·ÙˆÙŠØ±"
            )
        elif any(help_word in user_lower for help_word in ["Ù…Ø³Ø§Ø¹Ø¯Ø©", "Ø´Ù„ÙˆÙ†", "ÙƒÙŠÙ", "ÙˆÙŠÙ†", "Ø§Ø±ÙŠØ¯", "Ù…Ù…ÙƒÙ†"]):
            return (
                "Ø§ÙƒÙŠØ¯ Ø§ÙƒØ¯Ø± Ø§Ø³Ø§Ø¹Ø¯Ùƒ! ðŸ’ª\n\n"
                "Ù…Ù…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…:\n"
                "â“ /help - Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø©\n"
                "ðŸ’¬ /chat - Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø°ÙƒÙŠØ©\n"
                "ðŸŽ¨ /image - Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ±\n"
                "ðŸ“Š /create_poll - Ù„Ù„Ø§Ø³ØªØ·Ù„Ø§Ø¹Ø§Øª\n\n"
                "Ø§Ùˆ Ø§ÙƒØªØ¨Ù„ÙŠ Ù…Ø¨Ø§Ø´Ø±Ø© Ø´ØªØ±ÙŠØ¯ ÙˆØ§ÙƒØ¯Ø± Ø§Ø³Ø§Ø¹Ø¯Ùƒ!"
            )
        else:
            # Context-specific fallback instead of generic error
            return (
                "ÙÙ‡Ù…Øª Ø·Ù„Ø¨Ùƒ Ø¨Ø³ ØµØ§Ø± Ø®Ø·Ø£ Ù…Ø¤Ù‚Øª! ðŸ”„\n\n"
                "Ø¬Ø±Ø¨ Ù…Ø±Ø© Ø«Ø§Ù†ÙŠØ© Ø§Ùˆ Ø§Ø³ØªØ®Ø¯Ù…:\n"
                "ðŸ’¬ /chat - Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©\n"
                "â“ /help - Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©\n\n"
                "Ø§ÙƒØªØ¨Ù„ÙŠ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø®ØªÙ„ÙØ© ÙˆØ§ÙƒØ¯Ø± Ø§Ø³Ø§Ø¹Ø¯Ùƒ Ø§ÙƒØ«Ø±!"
            )
    
    async def generate_image(self, prompt: str, image_path: str) -> bool:
        """
        Generate an image using Gemini 2.0 Flash image generation.
        
        Args:
            prompt: Description of the image to generate
            image_path: Path where to save the generated image
            
        Returns:
            True if successful, False otherwise
        """
        try:
            # Check if Gemini is available
            if not self.gemini_client:
                logger.error("Gemini image generation unavailable - GEMINI_API_KEY not set")
                return False
            
            logger.info(f"Generating image with Gemini 2.0 Flash for prompt: {prompt[:50]}...")
            
            # Generate image with Gemini (note: direct image generation via API is limited)
            # For now, return False to indicate image generation is not available
            logger.warning("Direct image generation with Gemini API is limited")
            return False
            
            if response.candidates and response.candidates[0].content:
                content = response.candidates[0].content
                
                if content.parts:
                    for part in content.parts:
                        # Look for image data in the response
                        if part.inline_data and part.inline_data.data:
                            # Save the generated image
                            with open(image_path, 'wb') as f:
                                f.write(part.inline_data.data)
                            
                            logger.info("Successfully generated and saved image with Gemini")
                            return True
                        elif part.text:
                            # Log any text response from Gemini
                            logger.info(f"Gemini text response: {part.text[:100]}")
                
                logger.error("No image data found in Gemini response")
                return False
            else:
                logger.error("No content in Gemini response")
                return False
                
        except Exception as e:
            logger.error(f"Error generating image with Gemini: {e}")
            return False
    
    async def analyze_image(self, image_data: bytes, user_message: str = "") -> str:
        """
        Analyze an image using GPT-5 Chat Vision capabilities.
        
        Args:
            image_data: The image data as bytes
            user_message: Optional context message from user
            
        Returns:
            Analysis description in Iraqi Arabic
        """
        try:
            logger.info("Attempting image analysis with GPT-5 Chat Vision...")
            
            # Convert image to base64
            base64_image = base64.b64encode(image_data).decode('utf-8')
            
            # Create prompt for image analysis
            analysis_prompt = "Ø­Ù„Ù„ Ù‡Ø°Ù‡ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø§Ù„ØªÙØµÙŠÙ„ ÙˆØ§ÙˆØµÙÙ‡Ø§ Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ©. Ø§Ø°ÙƒØ± ÙƒÙ„ Ø´ÙŠ ØªØ´ÙˆÙÙ‡ ÙÙŠÙ‡Ø§ - Ø§Ù„Ø£Ù„ÙˆØ§Ù†ØŒ Ø§Ù„Ø£Ø´Ø®Ø§ØµØŒ Ø§Ù„Ù…ÙƒØ§Ù†ØŒ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ØŒ ÙˆØ§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ù‡Ù…Ø©."
            
            if user_message:
                analysis_prompt += f"\n\nØ§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù‚Ø§Ù„: {user_message}"
            
            # Analyze with GPT-5 Chat Vision
            response = self.gpt5_client.chat.completions.create(
                extra_headers={
                    "HTTP-Referer": "https://iraqi-bot.replit.app",
                    "X-Title": "Iraqi Dialect Bot - GPT-5",
                },
                model="openai/gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": self.system_instruction + "\n\nØ£Ù†Øª ØªØ­Ù„Ù„ ØµÙˆØ±Ø© Ø£Ø±Ø³Ù„Ù‡Ø§ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…. Ø§ÙˆØµÙ ÙƒÙ„ Ø´ÙŠ ØªØ´ÙˆÙÙ‡ Ø¨Ø§Ù„ØªÙØµÙŠÙ„ ÙˆØ§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ©."
                    },
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": analysis_prompt
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=500,
                temperature=0.7,
            )
            
            if response.choices and response.choices[0].message.content:
                result = response.choices[0].message.content.strip()
                # Clean the response to avoid formatting issues
                result = self._clean_response_text(result)
                logger.info("Successfully analyzed image with GPT-5 Chat Vision")
                return result
            else:
                logger.warning("Empty response from GPT-5 Chat Vision")
                # Try Gemini Vision as fallback for empty response
                if self.gemini_client:
                    try:
                        logger.info("Trying Gemini Vision as fallback for empty GPT-5 response...")
                        return await self._analyze_image_with_gemini(image_data, user_message)
                    except Exception as gemini_error:
                        logger.error(f"Gemini Vision also failed: {gemini_error}")
                return "Ù…Ø§ ÙƒØ¯Ø±Øª Ø£Ø­Ù„Ù„ Ø§Ù„ØµÙˆØ±Ø©ØŒ Ø¨Ø³ ØªÙƒØ¯Ø± ØªØ­Ú†ÙŠ ÙˆÙŠØ§ÙŠ Ø¹Ø§Ø¯ÙŠ."
                
        except Exception as e:
            logger.error(f"Error in GPT-5 Chat Vision analysis: {e}")
            
            # Try Gemini Vision as fallback
            if self.gemini_client:
                try:
                    logger.info("Trying Gemini Vision as fallback for image analysis...")
                    return await self._analyze_image_with_gemini(image_data, user_message)
                except Exception as gemini_error:
                    logger.error(f"Gemini Vision also failed: {gemini_error}")
            
            # Last resort: text-only fallback response
            fallback_prompt = f"Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø£Ø±Ø³Ù„ ØµÙˆØ±Ø© ÙˆÙ‚Ø§Ù„: {user_message if user_message else 'Ù…Ø§ Ù‚Ø§Ù„ Ø´ÙŠ'}. Ø±Ø¯ Ø¹Ù„ÙŠÙ‡ Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ© ÙˆØ§Ø¹ØªØ°Ø± Ù„Ù‡ Ø¥Ù†Ùƒ Ù…Ø§ ØªÙƒØ¯Ø± ØªØ´ÙˆÙ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø³ ØªÙƒØ¯Ø± ØªØ³Ø§Ø¹Ø¯Ù‡ Ø¨Ø·Ø±Ù‚ Ø«Ø§Ù†ÙŠØ©."
            
            try:
                response = self.gpt5_client.chat.completions.create(
                    extra_headers={
                        "HTTP-Referer": "https://iraqi-bot.replit.app",
                        "X-Title": "Iraqi Dialect Bot - GPT-5",
                    },
                    model="openai/gpt-4o",
                    messages=[
                        {
                            "role": "system",
                            "content": self.system_instruction
                        },
                        {
                            "role": "user",
                            "content": fallback_prompt
                        }
                    ],
                    max_tokens=500,
                    temperature=0.7,
                )
                
                if response.choices and response.choices[0].message.content:
                    return response.choices[0].message.content.strip()
                    
            except Exception as fallback_error:
                logger.error(f"Error in fallback response: {fallback_error}")
            
            return "Ù…Ø¹Ø°Ø±Ø©ØŒ Ù…Ø§ ÙƒØ¯Ø±Øª Ø£Ø­Ù„Ù„ Ø§Ù„ØµÙˆØ±Ø© Ù‡Ø³Ù‡. Ø¨Ø³ ØªÙƒØ¯Ø± ØªØ­Ú†ÙŠ ÙˆÙŠØ§ÙŠ Ø¹Ø§Ø¯ÙŠ ÙˆØ£Ø³Ø§Ø¹Ø¯Ùƒ Ø¨Ø£ÙŠ Ø´ÙŠ Ø«Ø§Ù†ÙŠ."
    
    async def _analyze_image_with_gemini(self, image_data: bytes, user_message: str = "") -> str:
        """
        Analyze an image using Gemini Vision capabilities as fallback.
        
        Args:
            image_data: The image data as bytes
            user_message: Optional context message from user
            
        Returns:
            Analysis description in Iraqi Arabic
        """
        try:
            logger.info("Using Gemini Vision for image analysis...")
            
            # Create prompt for image analysis in Iraqi dialect
            analysis_prompt = """Ø­Ù„Ù„ Ù‡Ø°Ù‡ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø§Ù„ØªÙØµÙŠÙ„ ÙˆØ§ÙˆØµÙÙ‡Ø§ Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ© Ø§Ù„Ø£ØµÙŠÙ„Ø©. 

Ø§Ø°ÙƒØ±:
- Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© 
- Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø£Ùˆ Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª (Ø¥Ø°Ø§ Ù…ÙˆØ¬ÙˆØ¯Ø©)
- Ø§Ù„Ù…ÙƒØ§Ù† Ø£Ùˆ Ø§Ù„Ø¨ÙŠØ¦Ø©
- Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙˆØ§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ù‡Ù…Ø©
- Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù„Ø§ÙØªØ© Ù„Ù„Ù†Ø¸Ø±
- Ø§Ù„Ø¬Ùˆ Ø§Ù„Ø¹Ø§Ù… Ù„Ù„ØµÙˆØ±Ø©

Ø§Ø³ØªØ®Ø¯Ù… ÙƒÙ„Ù…Ø§Øª Ø¹Ø±Ø§Ù‚ÙŠØ© Ù…Ø«Ù„: Ø´ÙƒÙˆ Ù…Ø§ÙƒÙˆØŒ Ø²ÙŠÙ†ØŒ Ø­Ù„ÙˆØŒ Ú†Ø§Ù†ØŒ Ù‡Ø³Ù‡ØŒ Ø´ÙˆÙŠØ©ØŒ ÙˆÙŠØ§Ù‡

ÙƒÙ† Ù…ÙØµÙ„ ÙˆÙˆØµÙ ÙƒÙ„ Ø´ÙŠ ØªØ´ÙˆÙÙ‡ Ø¨Ø·Ø±ÙŠÙ‚Ø© ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…ÙÙ‡ÙˆÙ…Ø©."""
            
            if user_message:
                analysis_prompt += f"\n\nØ§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù‚Ø§Ù„: {user_message}"
            
            # Convert image data to format Gemini can understand
            import base64
            base64_image = base64.b64encode(image_data).decode('utf-8')
            
            # Analyze with Gemini Vision
            model = genai.GenerativeModel('gemini-1.5-flash')
            import PIL.Image
            import io
            image = PIL.Image.open(io.BytesIO(image_data))
            response = model.generate_content(
                [analysis_prompt, image],
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=800,
                    temperature=0.7,
                )
            )
            
            # Better response handling for Gemini Vision
            if response and response.text:
                result = response.text.strip()
                # Clean the response to avoid formatting issues
                result = self._clean_response_text(result)
                logger.info("Successfully analyzed image with Gemini Vision")
                return result
            
            logger.warning("Empty response from Gemini Vision")
            return "Ù…Ø§ ÙƒØ¯Ø±Øª Ø£Ø­Ù„Ù„ Ø§Ù„ØµÙˆØ±Ø© Ø¨ÙˆØ§Ø³Ø·Ø© GeminiØŒ Ø¨Ø³ ØªÙƒØ¯Ø± ØªØ­Ú†ÙŠ ÙˆÙŠØ§ÙŠ Ø¹Ø§Ø¯ÙŠ."
                
        except Exception as e:
            logger.error(f"Error in Gemini Vision analysis: {e}")
            return "Ù…Ø¹Ø°Ø±Ø©ØŒ ØµØ§Ø± Ø®Ø·Ø£ ÙˆÙ‚Øª ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ù€ Gemini. ØªÙƒØ¯Ø± ØªØ­Ú†ÙŠ ÙˆÙŠØ§ÙŠ Ø¹Ø§Ø¯ÙŠ ÙˆØ£Ø³Ø§Ø¹Ø¯Ùƒ Ø¨Ø£ÙŠ Ø´ÙŠ Ø«Ø§Ù†ÙŠ."
    
    def _clean_response_text(self, text: str) -> str:
        """Clean response text to avoid Telegram formatting issues."""
        if not text:
            return text
            
        # Remove or escape problematic characters that break Telegram parsing
        # Remove excessive backticks and markdown characters
        import re
        
        # Remove multiple consecutive backticks
        text = re.sub(r'`{2,}', '`', text)
        
        # Remove unmatched backticks at the end
        if text.count('`') % 2 != 0:
            # Remove the last backtick if unmatched
            last_backtick = text.rfind('`')
            if last_backtick != -1:
                text = text[:last_backtick] + text[last_backtick+1:]
        
        # Clean up other problematic markdown characters
        # Remove unmatched asterisks
        if text.count('*') % 2 != 0:
            text = text.replace('*', '')
        
        # Remove unmatched underscores  
        if text.count('_') % 2 != 0:
            text = text.replace('_', '')
            
        # Ensure no broken markdown links
        text = re.sub(r'\[([^\]]*)\]\([^\)]*$', r'\1', text)  # Fix broken links
        
        return text.strip()
    
    async def translate_to_english(self, arabic_text: str) -> str:
        """
        Translate Arabic text to English using GPT-5 Chat.
        
        Args:
            arabic_text: Arabic text to translate
            
        Returns:
            English translation
        """
        try:
            prompt = f"Translate this Arabic text to natural English. Keep the meaning and tone:\n\n{arabic_text}"
            
            response = self.gpt5_client.chat.completions.create(
                extra_headers={
                    "HTTP-Referer": "https://iraqi-bot.replit.app",
                    "X-Title": "Iraqi Dialect Bot - GPT-5",
                },
                model="openai/gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": "You are a professional translator with enhanced GPT-5 capabilities. Translate Arabic to English accurately while preserving meaning, tone, and cultural context. Pay special attention to Iraqi dialect nuances."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                max_tokens=1000,
            )
            
            if response.choices and response.choices[0].message.content:
                result = response.choices[0].message.content.strip()
                logger.info("Successfully translated Arabic to English with GPT-5")
                return result
            else:
                return "Translation failed. Please try again."
                
        except Exception as e:
            logger.error(f"Error in Arabic to English translation: {e}")
            return "Sorry, translation failed. Please try again."
    
    async def translate_to_arabic(self, english_text: str) -> str:
        """
        Translate English text to Iraqi Arabic using GPT-5 Chat.
        
        Args:
            english_text: English text to translate
            
        Returns:
            Iraqi Arabic translation
        """
        try:
            prompt = f"Translate this English text to Iraqi Arabic dialect. Use authentic Iraqi expressions and vocabulary:\n\n{english_text}"
            
            response = self.gpt5_client.chat.completions.create(
                extra_headers={
                    "HTTP-Referer": "https://iraqi-bot.replit.app",
                    "X-Title": "Iraqi Dialect Bot - GPT-5",
                },
                model="openai/gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": self.system_instruction + "\n\nYou are using enhanced GPT-5 translation capabilities for superior English to Iraqi Arabic conversion."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.7,
                max_tokens=1000,
            )
            
            if response.choices and response.choices[0].message.content:
                result = response.choices[0].message.content.strip()
                logger.info("Successfully translated English to Iraqi Arabic with GPT-5")
                return result
            else:
                return "ÙØ´Ù„Øª Ø§Ù„ØªØ±Ø¬Ù…Ø©. Ø¬Ø±Ø¨ Ù…Ø±Ø© Ø«Ø§Ù†ÙŠØ©."
                
        except Exception as e:
            logger.error(f"Error in English to Arabic translation: {e}")
            return "Ù…Ø¹Ø°Ø±Ø©ØŒ ÙØ´Ù„Øª Ø§Ù„ØªØ±Ø¬Ù…Ø©. Ø¬Ø±Ø¨ Ù…Ø±Ø© Ø«Ø§Ù†ÙŠØ©."
    
    async def generate_creative_description(self, description: str) -> tuple:
        """
        Generate creative description with English prompt using GPT-5 Chat or Gemini.
        
        Args:
            description: User's description in Arabic
            
        Returns:
            Tuple of (arabic_description, english_prompt)
        """
        try:
            prompt = f"""
Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØ±ÙŠØ¯ ÙˆØµÙ Ø¥Ø¨Ø¯Ø§Ø¹ÙŠ Ù„Ù€: {description}

Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù…Ù†Ùƒ:
1. Ø¥Ù†Ø´Ø§Ø¡ Ù†Øµ Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ Ù…ÙØµÙ„ ÙˆØ¯Ù‚ÙŠÙ‚ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ± (ÙŠÙƒÙˆÙ† Ù‚Ø§Ø¨Ù„ Ù„Ù„Ù†Ø³Ø®)
2. ÙˆØµÙ Ø¥Ø¨Ø¯Ø§Ø¹ÙŠ Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ© ÙŠØµÙ Ù†ÙØ³ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø­ÙŠÙˆÙŠØ© ÙˆØ¬Ù…ÙŠÙ„Ø©

Ø§Ø¨Ø¯Ø£ Ø§Ù„Ø¬ÙˆØ§Ø¨ Ø¨Ù€ "ENGLISH_PROMPT:" Ø«Ù… Ø§Ù„Ù†Øµ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØŒ Ø«Ù… Ø³Ø·Ø± ÙØ§Ø±ØºØŒ Ø«Ù… "ARABIC_DESCRIPTION:" Ø«Ù… Ø§Ù„ÙˆØµÙ Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠ.
            """
            
            # Try GPT-5 first
            try:
                response = self.gpt5_client.chat.completions.create(
                    extra_headers={
                        "HTTP-Referer": "https://iraqi-bot.replit.app",
                        "X-Title": "Iraqi Dialect Bot - GPT-5",
                    },
                    model="openai/gpt-4o",
                    messages=[
                        {
                            "role": "system",
                            "content": self.system_instruction + "\n\nYou are using enhanced creative capabilities for superior content generation and detailed prompts."
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    temperature=0.8,
                    max_tokens=1000,
                )
                
                if response.choices and response.choices[0].message.content:
                    result = response.choices[0].message.content.strip()
                    logger.info("Successfully generated creative description with GPT-5")
                    return self._parse_creative_response(result, description)
                    
            except Exception as gpt_error:
                logger.error(f"GPT-5 failed for creative description: {gpt_error}")
                # Try Gemini fallback
                if self.gemini_client:
                    try:
                        response = self.gemini_client.models.generate_content(
                            model="gemini-2.0-flash",
                            contents=[
                                types.Content(role="user", parts=[types.Part(text=prompt)])
                            ],
                            config=types.GenerateContentConfig(
                                max_output_tokens=800,
                                temperature=0.8,
                            ),
                        )
                        
                        if response and hasattr(response, 'text') and response.text:
                            result = response.text.strip()
                            logger.info("Successfully generated creative description with Gemini")
                            return self._parse_creative_response(result, description)
                    except Exception as gemini_error:
                        logger.error(f"Gemini also failed for creative description: {gemini_error}")
            
            # Fallback creative response
            return self._create_fallback_creative_description(description)
                
        except Exception as e:
            logger.error(f"Error generating creative description: {e}")
            return self._create_fallback_creative_description(description)
    
    def _parse_creative_response(self, response: str, description: str) -> tuple:
        """Parse the AI response to extract English prompt and Arabic description."""
        try:
            if "ENGLISH_PROMPT:" in response and "ARABIC_DESCRIPTION:" in response:
                parts = response.split("ARABIC_DESCRIPTION:")
                english_part = parts[0].replace("ENGLISH_PROMPT:", "").strip()
                arabic_part = parts[1].strip()
                return (arabic_part, english_part)
            else:
                # If format is not as expected, create fallback
                return self._create_fallback_creative_description(description)
        except:
            return self._create_fallback_creative_description(description)
    
    def _create_fallback_creative_description(self, description: str) -> tuple:
        """Create fallback creative description when AI fails."""
        english_prompt = f"A beautiful, high-quality artistic image of {description}, professional photography, detailed, vibrant colors, excellent lighting"
        arabic_description = f"ØµÙˆØ±Ø© Ø­Ù„ÙˆØ© ÙˆÙ…ÙØµÙ„Ø© Ù„Ù€ {description} Ø¨Ø£Ù„ÙˆØ§Ù† Ø²Ø§Ù‡ÙŠØ© ÙˆØ¥Ø¶Ø§Ø¡Ø© Ù…Ù…ØªØ§Ø²Ø©ØŒ ØªØµÙˆÙŠØ± Ø§Ø­ØªØ±Ø§ÙÙŠ Ø¹Ø§Ù„ÙŠ Ø§Ù„Ø¬ÙˆØ¯Ø©"
        return (arabic_description, english_prompt)
    
    async def analyze_with_tools(self, prompt: str, analysis_type: str) -> str:
        """
        Advanced analysis using GPT-5 Chat enhanced capabilities.
        
        Args:
            prompt: Text to analyze
            analysis_type: Type of analysis to perform
            
        Returns:
            Analysis result in Iraqi Arabic
        """
        try:
            response = self.gpt5_client.chat.completions.create(
                extra_headers={
                    "HTTP-Referer": "https://iraqi-bot.replit.app",
                    "X-Title": "Iraqi Dialect Bot - GPT-5",
                },
                model="openai/gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": self.system_instruction + "\n\nYou have access to advanced tools and GPT-5 Chat's enhanced capabilities for superior analysis."
                    },
                    {
                        "role": "user",
                        "content": f"Ø­Ù„Ù„ Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…ØªÙ‚Ø¯Ù…Ø© ({analysis_type}):\n\n{prompt}"
                    }
                ],
                temperature=0.7,
                max_tokens=500,  # Further reduced to work within available credits
            )
            
            if response.choices and response.choices[0].message.content:
                return response.choices[0].message.content.strip()
            else:
                return "Ù…Ø¹Ø°Ø±Ø©ØŒ Ù…Ø§ ÙƒØ¯Ø±Øª Ø£Ø³ÙˆÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…. Ø¬Ø±Ø¨ Ù…Ø±Ø© Ø«Ø§Ù†ÙŠØ©."
                
        except Exception as e:
            logger.error(f"Error in advanced analysis: {e}")
            # Check if it's a credit/payment error
            if "402" in str(e) or "credit" in str(e).lower() or "payment" in str(e).lower():
                logger.warning("OpenRouter credits insufficient for advanced analysis")
                return "Ù…Ø¹Ø°Ø±Ø©ØŒ Ø§Ù„Ø£Ø±ØµØ¯Ø© Ù…Ùˆ ÙƒØ§ÙÙŠØ© Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù‡Ø³Ø©. Ø¬Ø±Ø¨ Ù…Ø±Ø© Ø«Ø§Ù†ÙŠØ© Ø¨Ø§Ø¬Ø±."
            return "Ù…Ø¹Ø°Ø±Ø©ØŒ ØµØ§Ø± Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…."
    
    async def generate_structured_response(self, prompt: str, response_type: str) -> dict:
        """
        Generate structured response using GPT-5 Chat.
        
        Args:
            prompt: User's prompt
            response_type: Type of structured response
            
        Returns:
            Dictionary with success status and content
        """
        try:
            response = self.gpt5_client.chat.completions.create(
                extra_headers={
                    "HTTP-Referer": "https://iraqi-bot.replit.app",
                    "X-Title": "Iraqi Dialect Bot - GPT-5",
                },
                model="openai/gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": self.system_instruction + f"\n\nGenerate a {response_type} structured response using GPT-5 Chat's enhanced capabilities."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.7,
                max_tokens=500,  # Further reduced to work within available credits
            )
            
            if response.choices and response.choices[0].message.content:
                return {
                    "success": True,
                    "content": response.choices[0].message.content.strip()
                }
            else:
                return {
                    "success": False,
                    "content": "Ù…Ø§ ÙƒØ¯Ø±Øª Ø£Ø³ÙˆÙŠ Ø§Ù„Ø±Ø¯ Ø§Ù„Ù…Ù†Ø¸Ù…. Ø¬Ø±Ø¨ Ù…Ø±Ø© Ø«Ø§Ù†ÙŠØ©."
                }
                
        except Exception as e:
            logger.error(f"Error in structured response: {e}")
            # Check if it's a credit/payment error
            if "402" in str(e) or "credit" in str(e).lower() or "payment" in str(e).lower():
                logger.warning("OpenRouter credits insufficient for structured response")
                return {
                    "success": False,
                    "content": "Ù…Ø¹Ø°Ø±Ø©ØŒ Ø§Ù„Ø£Ø±ØµØ¯Ø© Ù…Ùˆ ÙƒØ§ÙÙŠØ© Ù„Ù„Ø±Ø¯ Ø§Ù„Ù…Ù†Ø¸Ù… Ù‡Ø³Ø©. Ø¬Ø±Ø¨ Ù…Ø±Ø© Ø«Ø§Ù†ÙŠØ© Ø¨Ø§Ø¬Ø±."
                }
            return {
                "success": False,
                "content": "ØµØ§Ø± Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø±Ø¯ Ø§Ù„Ù…Ù†Ø¸Ù…."
            }
    
    @property
    def openai_api_key(self) -> str:
        """Compatibility property for image generation checks."""
        return self.gemini_api_key or ""